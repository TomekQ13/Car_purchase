{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from scipy import stats\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear():\n",
    "    os.system('cls')\n",
    "\n",
    "def ListNoDups(mylist):\n",
    "    mylist = list(dict.fromkeys(mylist))\n",
    "    return mylist\n",
    "\n",
    "def CleanList(list_to_clean, len_less_than = 3):\n",
    "    cleaned_list = [x for x in list_to_clean if str(x) != 'nan' and len(x) >= len_less_than ]\n",
    "    return clean_list\n",
    "\n",
    "class CarData:\n",
    "    \n",
    "    missing = -1\n",
    "    duplicates = -1\n",
    "    \n",
    "    price_outliers = -1\n",
    "    mileage_outliers = -1\n",
    "    year_outliers = -1\n",
    "    total_discard = -1\n",
    "    \n",
    "    corpus = []\n",
    "    def __init__(self, path, price_outlier_mt = 200000, mileage_outlier_mt = 400000,\n",
    "                year_outlier_lt = 1995, engine_outlier_mt = 4000, engine_outlier_lt = 750,\n",
    "                dependent_variable = 'price',\n",
    "                categorical_variables = ['engine_type', 'city', 'province'],\n",
    "                numeric_variables = ['price', 'mileage_km', 'engine_cm3', 'year']):\n",
    "        \n",
    "        #define outliers values\n",
    "        self.price_outlier_mt = price_outlier_mt\n",
    "        self.mileage_outlier_mt = mileage_outlier_mt\n",
    "        self.year_outlier_lt =  year_outlier_lt\n",
    "        self.engine_outlier_mt = engine_outlier_mt\n",
    "        self.engine_outlier_lt = engine_outlier_lt\n",
    "        self.dependent_variable = dependent_variable\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        #define variable data types\n",
    "        self.numeric_variables = numeric_variables\n",
    "        self.categorical_variables = categorical_variables\n",
    "        \n",
    "        #read all .csv files from the directory\n",
    "        self.data = pd.concat(map(pd.read_csv, glob.glob(os.path.join(path, \"*.csv\"))), sort=False)\n",
    "        \n",
    "        #drop the duplicates and save the number of duplicates - many duplicates due to data gathering method\n",
    "        self.duplicates = len(self.data) - len(self.data.drop_duplicates())\n",
    "        self.data.drop_duplicates(inplace = True)\n",
    "        \n",
    "        #rename columns\n",
    "        self.data.columns = ['title', 'price', 'sub_title', 'mileage_km', 'year', 'engine_cm3',\n",
    "                'engine_type', 'city', 'province', 'negotiable']\n",
    "        \n",
    "        #drop NaNs and save the number of rows dropped to the missing varaible\n",
    "        self.missing = self.data['engine_type'].isna().sum()\n",
    "        self.data.dropna(subset = ['engine_type'], axis = 'index', inplace = True)\n",
    "        \n",
    "        self.missing = self.missing + self.data['city'].isna().sum()\n",
    "        self.data.dropna(subset = ['city'], axis = 'index', inplace = True)\n",
    "        \n",
    "        self.missing = self.missing + self.data['engine_cm3'].isna().sum()\n",
    "        self.data.dropna(subset = ['engine_cm3'], axis = 'index', inplace = True)\n",
    "        \n",
    "        #clean up the columns\n",
    "        self.data['price'] = self.data['price'].apply(lambda x: x.replace(\",\", \"\"))\n",
    "        self.data['price'] = self.data['price'].apply(lambda x: x.replace(\" \", \"\")).astype('int')\n",
    "        \n",
    "        self.data['mileage_km'] = self.data['mileage_km'].apply(lambda x: x.replace(\"km\", \"\"))\n",
    "        self.data['mileage_km'] = self.data['mileage_km'].apply(lambda x: x.replace(\" \", \"\")).astype('float')\n",
    "        \n",
    "        self.data['engine_cm3'] = self.data['engine_cm3'].astype('str')\n",
    "        self.data['engine_cm3'] = self.data['engine_cm3'].apply(lambda x: x.replace('cm3', ''))\n",
    "        self.data['engine_cm3'] = self.data['engine_cm3'].apply(lambda x: x.replace(' ','')).astype('int')\n",
    "        \n",
    "        self.data['province'] = self.data['province'].astype('str')\n",
    "        self.data['province'] = self.data['province'].apply(lambda x: x.replace('(',''))\n",
    "        self.data['province'] = self.data['province'].apply(lambda x: x.replace(')',''))\n",
    "        \n",
    "        self.data['sub_title'] = self.data['sub_title'].astype('str') #may change that in the future - possible info loss due to lowercase\n",
    "        \n",
    "        \n",
    "        self.data['title'] = self.data['title'].astype('str') #may change that in the future - possible info loss due to lowercase\n",
    "        \n",
    "        self.data['negotiable'] = self.data['negotiable'].astype('str')\n",
    "        \n",
    "        #Add ID column\n",
    "        self.data.insert(loc = 0, column = 'ID', value = range(1, len(self.data)+1))\n",
    "\n",
    "        #discard outliers and calculate the numbers\n",
    "        self.total_discard = len(self.data) - len(self.data[(self.data['price'] <= self.price_outlier_mt) &\n",
    "                                                        (self.data['mileage_km'] <= self.mileage_outlier_mt) &\n",
    "                                                        (self.data['year'] >= self.year_outlier_lt) &\n",
    "                                                        (self.data['engine_cm3'] <= self.engine_outlier_mt) &\n",
    "                                                        (self.data['engine_cm3'] >= self.engine_outlier_lt)])        \n",
    "        \n",
    "        self.price_outliers = len(self.data[self.data['price'] > price_outlier_mt])\n",
    "        self.data = self.data[self.data['price'] <= price_outlier_mt]\n",
    "        \n",
    "        self.mileage_outliers = len(self.data[self.data['mileage_km'] > mileage_outlier_mt])\n",
    "        self.data = self.data[self.data['mileage_km'] <= mileage_outlier_mt]\n",
    "        \n",
    "        self.year_outliers = len(self.data[self.data['year'] < year_outlier_lt])\n",
    "        self.data = self.data[self.data['year'] >= year_outlier_lt]\n",
    "        \n",
    "        self.engine_outliers = len(self.data[(self.data['engine_cm3'] > engine_outlier_mt) |\n",
    "                                            (self.data['engine_cm3'] < engine_outlier_lt)])\n",
    "        self.data = self.data[(self.data['engine_cm3'] <= engine_outlier_mt) & \n",
    "                             (self.data['engine_cm3'] >= engine_outlier_lt)]\n",
    "        \n",
    "        #NLP\n",
    "        self.data['concat_title_subtitle'] = self.data['title'] + ' ' + self.data['sub_title']\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.lower())\n",
    "        \n",
    "        #replace problematic cases for NLP for title and subtitle\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('+',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('(',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace(')',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('**',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('*',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace(']',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('[',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace(\"/\",\" \"))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace(\"\\\\\",\" \"))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace(',',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('?',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('.',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('!',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('_',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('-',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('|',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('#',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('%',' '))\n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('~',' '))  \n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('*',' ')) \n",
    "        self.data['concat_title_subtitle'] = self.data['concat_title_subtitle'].apply(lambda x: x.replace('*',' '))\n",
    "        \n",
    "        #NLP preprocessing for location\n",
    "        self.data['province'] = self.data['province'].apply(lambda x: x.lower()) \n",
    "        self.data['province'] = self.data['province'].apply(lambda x: x.replace('ą', 'a')) \n",
    "        self.data['province'] = self.data['province'].apply(lambda x: x.replace('ę', 'e'))\n",
    "        self.data['province'] = self.data['province'].apply(lambda x: x.replace('ł', 'l'))\n",
    "        self.data['province'] = self.data['province'].apply(lambda x: x.replace('ś', 's'))\n",
    "        self.data['province'] = self.data['province'].apply(lambda x: x.replace('ć', 'c'))\n",
    "        self.data['province'] = self.data['province'].apply(lambda x: x.replace('ż', 'z'))\n",
    "        \n",
    "    def describe(self):\n",
    "        #descriptive statistice\n",
    "        desc_stats = round(pd.DataFrame(\n",
    "                        data = self.data[self.numerical_variables].describe(),\n",
    "                        columns = self.data[self.numerical_variables].columns),2)\n",
    "        return desc_stats\n",
    "    \n",
    "    def outliers(self):\n",
    "        #baisc data about outliers discarded during preprocessing\n",
    "        print('Offers with price greater than '+str(self.price_outlier_mt)+' have been discarded')\n",
    "        print('The number of such offers = '+str(self.price_outliers))\n",
    "        print('')\n",
    "        print('Offers with mileage greater than '+str(self.mileage_outlier_mt)+' have been discarded')\n",
    "        print('The number of such offers = '+str(self.mileage_outliers))\n",
    "        print('')\n",
    "        print('Offers with year lower than '+str(self.year_outlier_lt)+' have been discarded')\n",
    "        print('The number of such offers = '+str(self.year_outliers))\n",
    "        print('')\n",
    "        print('Offers with engine_cm3 greater than '+str(self.engine_outlier_lt)+\n",
    "              ' and lower than '+str(self.engine_outlier_mt)+' have been discarded')\n",
    "        print('The number of such offers = '+str(self.engine_outliers))\n",
    "        print('')\n",
    "        print('Total number of discarded offers = '+str(self.total_discard)\n",
    "              +'('+str(round(self.total_discard/len(self.data)*100,2))+'%)'\n",
    "              +' - may be different to the sum of above due to overlap')\n",
    "        \n",
    "    def scatter_nox(self, var = 'all', figsize_1 = 7, figsize_2 = 5):\n",
    "        #prints scatter plots with no x axis - a dummy sequence as x axis\n",
    "        if var != 'all' and var not in self.data.columns:\n",
    "            print('Variable not found in the dataset')\n",
    "        if var == 'all':\n",
    "            plt.rcParams[\"figure.figsize\"] = (figsize_1,figsize_2)\n",
    "            plt.scatter(y = self.data['mileage_km'], x = range(1, len(self.data)+1), s=1)\n",
    "            plt.title('mileage_km')\n",
    "            plt.show()\n",
    "\n",
    "            plt.scatter(y = self.data['price'], x = range(1, len(self.data)+1), s=1)\n",
    "            plt.title('price')\n",
    "            plt.show()\n",
    "\n",
    "            plt.scatter(y = self.data['year'], x = range(1, len(self.data)+1), s=1)\n",
    "            plt.title('year')\n",
    "            plt.show()\n",
    "\n",
    "            plt.scatter(y = self.data['engine_cm3'], x = range(1, len(self.data)+1), s=1)\n",
    "            plt.title('engine_cm3')\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.rcParams[\"figure.figsize\"] = (figsize_1,figsize_2)\n",
    "            plt.scatter(y = self.data[var], x = range(1, len(self.data)+1), s=1)\n",
    "            plt.title(var)\n",
    "            plt.show()\n",
    "            \n",
    "    def scatter(self, var = 'all'):\n",
    "        #prints scatter plots for numerical variables\n",
    "        if var != 'all' and var not in self.data.columns:\n",
    "            print('Variable not found in the dataset')\n",
    "        list_comb = []\n",
    "        if var == 'all':\n",
    "            for variable1 in enumerate(self.numeric_variables):\n",
    "                for variable2 in enumerate(self.numeric_variables):\n",
    "                    if variable1 != variable2 and variable1[1]+variable2[1] not in list_comb and variable2[1]+variable1[1] not in list_comb:\n",
    "                        plt.scatter(y = self.data[variable1[1]], x = self.data[variable2[1]], s=1)\n",
    "                        plt.title(\"Correlation between \"+variable1[1]+' and '+variable2[1])\n",
    "                        plt.ylabel(variable1[1])\n",
    "                        plt.xlabel(variable2[1])\n",
    "                        plt.show()\n",
    "                        list_comb.append(variable1[1]+variable2[1])\n",
    "        #else: - TO DO\n",
    "    \n",
    "    def hist(self, var = 'all', bins = 50):\n",
    "        if var != 'all' and var not in self.data.columns:\n",
    "            print('Variable not found in the dataset')\n",
    "        #prints histograms for numeric variables\n",
    "        if var == 'all':\n",
    "            for variable in enumerate(self.numeric_variables):\n",
    "                plt.hist(x = self.data[variable[1]], bins = bins)\n",
    "                plt.title(variable[1])\n",
    "                plt.show()\n",
    "     \n",
    "    def price_cat_vars(self, variables = '_NULL_'):\n",
    "        \n",
    "        if variables == '_NULL_':\n",
    "            variables = self.categorical_variables\n",
    "            \n",
    "        for variable in enumerate(variables):\n",
    "            # shows desrptive statistics of categorical variables\n",
    "            print(self.data.groupby(self.data[variable[1]])['price'].describe())\n",
    "            #the variables need further preprocessing\n",
    "            \n",
    "    def add_dummies(self, categorical_list, columns_to_check, delete_from_strings = 'yes', delete_column = 'no'):\n",
    "        #adds dummmies from cat_list, checks in every column of columns_to_check\n",
    "        for column in enumerate(columns_to_check):\n",
    "            for category in enumerate(categorical_list):\n",
    "                col_name = column[1] + '_' + category[1]\n",
    "                self.data[col_name] = self.data[column[1]].str.contains(category[1]).astype('int')\n",
    "                \n",
    "                #append newly craeted varaibles to categorical variables\n",
    "                if self.data[col_name].sum() > 0:\n",
    "                    self.categorical_variables.append(col_name)\n",
    "                else:\n",
    "                    self.data.drop(columns = [col_name], inplace = True)\n",
    "                \n",
    "                #delete the string from the column\n",
    "                if delete_from_strings == 'yes':\n",
    "                    self.data[column[1]] = self.data[column[1]].apply(lambda x: x.replace(category[1], ''))\n",
    "                    \n",
    "                if delete_column == 'yes':\n",
    "                    self.data.drop(columns = column, inplace = True)\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def add_dummies2(self, categorical_list, delete_from_column = 'yes', delete_concat_column = 'no', column = 'concat_title_subtitle'):\n",
    "        #adds dummmies from cat_list, checks in concat_title_subtitle column\n",
    "        for category in enumerate(categorical_list):\n",
    "            \n",
    "            #print(category)\n",
    "            \n",
    "            col_name = category[1]\n",
    "            self.data[col_name] = self.data[column].str.contains(category[1]).astype('int')\n",
    "                \n",
    "                #append newly craeted varaibles to categorical variables\n",
    "            if self.data[col_name].sum() > 0:\n",
    "                self.categorical_variables.append(col_name)\n",
    "            else:\n",
    "                self.data.drop(columns = [col_name], inplace = True)\n",
    "                \n",
    "                #delete the string from the column\n",
    "            if delete_from_column == 'yes':\n",
    "                self.data[column] = self.data[column].apply(lambda x: x.replace(category[1], ''))\n",
    "                                                \n",
    "            if delete_column == 'yes':\n",
    "                self.data.drop(columns = column, inplace = True)\n",
    "                \n",
    "        return self\n",
    "    \"\"\"\n",
    "    def ind_test(self, var, alpha = 0.05):\n",
    "        if alpha > 1 or alpha < 0:\n",
    "            print('Incorrect alpha value. Select a value from <0;1>.')\n",
    "            \n",
    "        if var != 'all' and var not in self.data.columns:\n",
    "            print('Variable not found in the dataset')\n",
    "        pivot = round(self.data.pivot_table(values = 'price', index = var, aggfunc = ['count', 'mean']),2)\n",
    "        pivot.columns = ['count', 'mean']\n",
    "        \n",
    "        mean_price = self.data['price'].mean()\n",
    "        \n",
    "        pivot['sm'] = pivot['mean']/((pivot['count'])**(1/2))\n",
    "        \n",
    "        pivot['t'] = (pivot['mean']-mean_price)/pivot['sm']\n",
    "        pivot['df'] = pivot['count']-1\n",
    "\n",
    "        #calculate p-value\n",
    "        pivot['t_border'] = stats.t.ppf(1-alpha/2, pivot['df'])\n",
    "        \n",
    "        #implementation here is not 100% mathematically correct\n",
    "        return pivot\n",
    "    \"\"\"\n",
    "    def anova(self, var = 'all', alpha = 0.05):\n",
    "        if var == 'all':\n",
    "            for variable in enumerate(self.categorical_variables):\n",
    "                anova_data = self.data[[variable[1], 'price']].reset_index().copy()\n",
    "                anova_data.columns = ['index', variable[1], 'price']\n",
    "                equation_string = 'price ~ '+str(variable[1])\n",
    "                model = ols(equation_string, data=anova_data).fit()\n",
    "                anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "                print(anova_table)\n",
    "                print()\n",
    "                \n",
    "                #pairwise comparisons\n",
    "                pairwise_comparison = pairwise_tukeyhsd(endog = anova_data['price'],\n",
    "                                                        groups = anova_data[variable[1]],\n",
    "                                                        alpha = alpha)\n",
    "                print(pairwise_comparison)\n",
    "                print()\n",
    "        #else:\n",
    "            #TO DO\n",
    "            #also TO DO check Anova assumptions\n",
    "    def make_corpus(self, column = 'concat_title_subtitle'):\n",
    "        #creates a corpus out of title and subtitle column\n",
    "        for i in range(0, len(self.data)):\n",
    "            #string = re.sub('[^a-zA-Z]', ' ', self.data.reset_index().loc[i]['concat_title_subtitle'])\n",
    "            string = self.data.reset_index().loc[i][column]\n",
    "            string = string.split()\n",
    "            self.corpus = self.corpus + string\n",
    "        self.corpus = ListNoDups(self.corpus)\n",
    "        \n",
    "        #with open(\"corpus.txt\", \"w\") as output:\n",
    "         #   output.write(str(self.corpus))\n",
    "    \n",
    "        return self.corpus\n",
    "    \n",
    "    def analyse_variables(self, list_of_variables, discard = 0.01):\n",
    "    # independence tests for a list of variable e.g. corpus\n",
    "        final_df = pd.DataFrame(columns = ['variable', 'mean_1', 'mean_0', 'count_1', 'count_0'])\n",
    "\n",
    "       # for variable in enumerate(list_of_variables):\n",
    "            #debug\n",
    "            #print(str(variable)+' done')\n",
    "            \n",
    "            self.data[variable[1]] = self.data['concat_title_subtitle'].str.contains(variable[1]).astype('int')\n",
    "\n",
    "            mean_1 = self.data.loc[self.data[variable[1]] == 1][self.dependent_variable].mean()\n",
    "            mean_0 = self.data.loc[self.data[variable[1]] == 0][self.dependent_variable].mean()\n",
    "\n",
    "            count_1 = len(self.data.loc[self.data[variable[1]] == 1])\n",
    "            count_0 = len(self.data.loc[self.data[variable[1]] == 0])        \n",
    "            \n",
    "            if count_1 >= discard * len(self.data) and count_0 >= discard * len(self.data):            \n",
    "                dict_to_append = {\n",
    "                    'variable' : variable[1],\n",
    "                    'mean_1' : mean_1,\n",
    "                    'mean_0' : mean_0,\n",
    "                    'count_1' : count_1,\n",
    "                    'count_0' : count_0\n",
    "                }\n",
    "\n",
    "                final_df = final_df.append(dict_to_append, ignore_index = True)\n",
    "\n",
    "            self.data.drop(columns = [variable[1]], inplace = True)\n",
    "\n",
    "            #if variable[0] % 1000 == 0:\n",
    "            #    print(str(variable[0])+'/'+str(len(list_of_variables)))\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        final_df['mean_diff'] = abs(final_df['mean_1'] - final_df['mean_0'])\n",
    "        final_df = final_df.sort_values(by = 'mean_diff', ascending = False).reset_index()\n",
    "        final_df.drop(columns = 'index', inplace = True)\n",
    "        \n",
    "        date = datetime.date(datetime.now())\n",
    "        \n",
    "        final_df.to_csv('analyse_variables_'+date+'.csv')\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "#cv = CountVectorizer(max_features = 1000)\n",
    "#X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CarData(path = 'data/', dependent_variable = 'price', categorical_variables = ['engine_type', 'city', 'province'],\n",
    "           numeric_variables = ['price', 'mileage_km', 'engine_cm3', 'year'])\n",
    "\n",
    "brands = ['alfa', 'audi', 'bmw', 'chevrolet', 'chrysler',\n",
    "          'citroen', 'dacia', 'daewoo', 'dodge', 'fiat',\n",
    "          'ford', 'honda', 'hyundai', 'jaguar', 'jeep',\n",
    "         'kia', 'rover', 'lexus', 'mazda', 'mercedes',\n",
    "         'mitsubishi', 'nissan', 'opel', 'peugeot',\n",
    "         'porsche', 'renault', 'seat', 'smart', 'subaru', \n",
    "          'suzuki', 'tesla', 'toyota', 'volkswagen',\n",
    "          'volvo', 'skoda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CarData at 0x1a732128dc8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.add_dummies2(brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = x.make_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = x.analyse_variables(corpus, discard = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis = pd.read_csv('analyse_variables_results.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = analysis['variable'].values.tolist()\n",
    "cleaned_variables = CleanList(list_to_clean = variables, len_less_than = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CarData at 0x1a732128dc8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.add_dummies2(cleaned_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-4ae39ab54612>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "date = datetime.date(datetime.now())\n",
    "data.data.to_csv('data_'+date+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_dummies(data.data['engine_type'].unique().tolist(), columns_to_check = ['engine_type'],  delete_column = 'yes')\n",
    "data.add_dummies(data.data['province'].unique().tolist(), column_to_check = ['province'],  delete_column = 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.data['price'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([data.data.iloc[: , [4,5,6]], data.data.iloc[: , 12:319]], axis = 1, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_scale = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = stand_scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.32876379,  0.42308547, -0.2643831 , ..., -0.0039582 ,\n",
       "        -0.01725583, -0.01187535],\n",
       "       [ 0.21901376, -0.39734962, -0.2643831 , ..., -0.0039582 ,\n",
       "        -0.01725583, -0.01187535],\n",
       "       [ 0.0390187 ,  0.42720826, -0.2643831 , ..., -0.0039582 ,\n",
       "        -0.01725583, -0.01187535],\n",
       "       ...,\n",
       "       [-0.05097883, -0.81375134, -0.2643831 , ..., -0.0039582 ,\n",
       "        -0.01725583, -0.01187535],\n",
       "       [ 0.5310052 ,  0.41690128, -0.2643831 , ..., -0.0039582 ,\n",
       "        -0.01725583, -0.01187535],\n",
       "       [-1.56776123, -0.81993553, -0.2643831 , ..., -0.0039582 ,\n",
       "        -0.01725583, -0.01187535]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lin_reg.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lin_reg.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [63828, 67886]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-bd652d57a653>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2094\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2096\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [63828, 67886]"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
